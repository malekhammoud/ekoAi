// Handwriting Recognition Service
import { AI_CONFIG } from '../config/ai';

interface Point {
  x: number;
  y: number;
}

interface Stroke {
  points: Point[];
  color: string;
  width: number;
  timestamp: number;
}

export class HandwritingService {
  private static instance: HandwritingService;
  
  static getInstance(): HandwritingService {
    if (!HandwritingService.instance) {
      HandwritingService.instance = new HandwritingService();
    }
    return HandwritingService.instance;
  }

  // Method 1: Google Cloud Vision API (Most Accurate)
  async recognizeWithGoogleVision(imageData: string): Promise<string> {
    try {
      console.log('üîç Using Google Vision API for handwriting recognition');
      
      if (!AI_CONFIG.google.apiKey || AI_CONFIG.google.apiKey === 'your_google_ai_api_key_here') {
        throw new Error('Google Vision API key not configured');
      }
      
      const response = await fetch(
        `https://vision.googleapis.com/v1/images:annotate?key=${AI_CONFIG.google.apiKey}`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            requests: [{
              image: {
                content: imageData.split(',')[1] // Remove data:image/png;base64, prefix
              },
              features: [{
                type: 'DOCUMENT_TEXT_DETECTION',
                maxResults: 1
              }]
            }]
          })
        }
      );

      if (!response.ok) {
        const errorText = await response.text();
        console.error('Google Vision API error response:', errorText);
        throw new Error(`Google Vision API error: ${response.status} - ${errorText}`);
      }

      const data = await response.json();
      console.log('Google Vision API response:', data);
      
      const text = data.responses?.[0]?.fullTextAnnotation?.text || '';
      
      console.log('‚úÖ Google Vision recognized text:', text);
      return text.trim();
    } catch (error) {
      console.error('‚ùå Google Vision API error:', error);
      throw error;
    }
  }

  // Method 2: AI-based recognition using Gemini Vision
  async recognizeWithAI(imageData: string): Promise<string> {
    try {
      console.log('üîç Using AI (Gemini) for handwriting recognition');
      
      if (!AI_CONFIG.google.apiKey || AI_CONFIG.google.apiKey === 'your_google_ai_api_key_here') {
        throw new Error('Google AI API key not configured');
      }
      
      const response = await fetch(
        `${AI_CONFIG.google.baseURL}/models/gemini-pro-vision:generateContent?key=${AI_CONFIG.google.apiKey}`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            contents: [{
              parts: [
                {
                  text: "Please read and transcribe any handwritten text you see in this image. Return only the text content, no explanations or additional commentary. If you cannot read any text, return an empty string."
                },
                {
                  inline_data: {
                    mime_type: "image/png",
                    data: imageData.split(',')[1]
                  }
                }
              ]
            }],
            generationConfig: {
              temperature: 0.1,
              maxOutputTokens: 200
            }
          })
        }
      );

      if (!response.ok) {
        const errorText = await response.text();
        console.error('Gemini Vision API error response:', errorText);
        throw new Error(`Gemini Vision API error: ${response.status} - ${errorText}`);
      }

      const data = await response.json();
      console.log('Gemini Vision API response:', data);
      
      const text = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
      
      console.log('‚úÖ AI recognized text:', text);
      return text.trim();
    } catch (error) {
      console.error('‚ùå AI recognition error:', error);
      throw error;
    }
  }

  // Method 3: Tesseract.js OCR (Client-side OCR)
  async recognizeWithTesseract(canvas: HTMLCanvasElement): Promise<string> {
    try {
      console.log('üîç Using Tesseract.js for OCR recognition');
      
      // For now, we'll use a simple pattern-based approach
      // In a real implementation, you would use Tesseract.js here
      const imageData = canvas.toDataURL('image/png');
      
      // This is a placeholder - in production you'd use:
      // const { createWorker } = await import('tesseract.js');
      // const worker = await createWorker();
      // const { data: { text } } = await worker.recognize(imageData);
      // await worker.terminate();
      // return text;
      
      throw new Error('Tesseract.js not implemented - would require additional dependency');
    } catch (error) {
      console.error('‚ùå Tesseract OCR error:', error);
      throw error;
    }
  }

  // Method 4: Enhanced Pattern Recognition (Fallback)
  recognizeWithPatterns(strokes: Stroke[]): string {
    console.log('üîç Using enhanced pattern recognition as fallback');
    
    if (strokes.length === 0) return '';

    // Filter out eraser strokes
    const drawStrokes = strokes.filter(s => s.color !== 'transparent');
    
    if (drawStrokes.length === 0) return '';

    // Analyze stroke patterns for basic character recognition
    const analysis = this.analyzeStrokesForCharacters(drawStrokes);
    const recognizedText = this.strokesToCharacters(analysis, drawStrokes);
    
    console.log('‚úÖ Pattern recognition result:', recognizedText);
    return recognizedText;
  }

  // Main recognition method that tries multiple approaches
  async recognizeHandwriting(strokes: Stroke[], canvas?: HTMLCanvasElement): Promise<string> {
    console.log('üéØ Starting handwriting recognition with', strokes.length, 'strokes');

    if (strokes.length === 0) {
      return '';
    }

    // Filter out eraser strokes for recognition
    const drawStrokes = strokes.filter(s => s.color !== 'transparent');
    if (drawStrokes.length === 0) {
      return '';
    }

    // Try recognition methods in order of accuracy
    const methods = [
      {
        name: 'Google Vision API',
        enabled: !!AI_CONFIG.google.apiKey && AI_CONFIG.google.apiKey !== 'your_google_ai_api_key_here',
        method: () => this.tryGoogleVision(canvas)
      },
      {
        name: 'Gemini Vision AI',
        enabled: !!AI_CONFIG.google.apiKey && AI_CONFIG.google.apiKey !== 'your_google_ai_api_key_here',
        method: () => this.tryAIVision(canvas)
      },
      {
        name: 'Enhanced Pattern Recognition',
        enabled: true,
        method: () => Promise.resolve(this.recognizeWithPatterns(drawStrokes))
      }
    ];

    for (const { name, enabled, method } of methods) {
      if (!enabled) {
        console.log(`‚è≠Ô∏è Skipping ${name} - not configured`);
        continue;
      }

      try {
        console.log(`üîÑ Trying ${name}...`);
        const result = await method();
        
        if (result && result.trim().length > 0) {
          console.log(`‚úÖ ${name} recognition successful:`, result);
          return result;
        } else {
          console.log(`‚ö†Ô∏è ${name} returned empty result`);
        }
      } catch (error) {
        console.warn(`‚ö†Ô∏è ${name} recognition failed:`, error);
        continue;
      }
    }

    // If all methods fail, return a helpful message
    console.log('‚ö†Ô∏è All recognition methods failed, returning fallback message');
    return 'Recognition in progress... Please ensure clear handwriting for better results.';
  }

  private async tryGoogleVision(canvas?: HTMLCanvasElement): Promise<string> {
    if (!canvas) {
      throw new Error('Canvas not available for Google Vision');
    }
    
    const imageData = canvas.toDataURL('image/png');
    return this.recognizeWithGoogleVision(imageData);
  }

  private async tryAIVision(canvas?: HTMLCanvasElement): Promise<string> {
    if (!canvas) {
      throw new Error('Canvas not available for AI Vision');
    }
    
    const imageData = canvas.toDataURL('image/png');
    return this.recognizeWithAI(imageData);
  }

  // Enhanced stroke analysis for character recognition
  private analyzeStrokesForCharacters(strokes: Stroke[]) {
    const analysis = {
      strokeCount: strokes.length,
      totalPoints: 0,
      boundingBoxes: [] as any[],
      strokeDirections: [] as string[],
      strokeLengths: [] as number[],
      intersections: 0,
      closedShapes: 0
    };

    strokes.forEach((stroke, index) => {
      analysis.totalPoints += stroke.points.length;
      
      // Calculate bounding box for each stroke
      const bbox = this.getStrokeBoundingBox(stroke.points);
      analysis.boundingBoxes.push(bbox);
      
      // Analyze stroke direction and length
      const direction = this.getDetailedStrokeDirection(stroke.points);
      const length = this.getStrokeLength(stroke.points);
      
      analysis.strokeDirections.push(direction);
      analysis.strokeLengths.push(length);
      
      // Check if stroke forms a closed shape
      if (this.isClosedShape(stroke.points)) {
        analysis.closedShapes++;
      }
    });

    // Analyze intersections between strokes
    analysis.intersections = this.countStrokeIntersections(strokes);
    
    console.log('üìä Character analysis:', analysis);
    return analysis;
  }

  private getStrokeBoundingBox(points: Point[]) {
    return {
      minX: Math.min(...points.map(p => p.x)),
      minY: Math.min(...points.map(p => p.y)),
      maxX: Math.max(...points.map(p => p.x)),
      maxY: Math.max(...points.map(p => p.y))
    };
  }

  private getDetailedStrokeDirection(points: Point[]): string {
    if (points.length < 2) return 'point';
    
    const start = points[0];
    const end = points[points.length - 1];
    const deltaX = end.x - start.x;
    const deltaY = end.y - start.y;
    
    const angle = Math.atan2(deltaY, deltaX) * 180 / Math.PI;
    
    if (angle >= -22.5 && angle < 22.5) return 'right';
    if (angle >= 22.5 && angle < 67.5) return 'down-right';
    if (angle >= 67.5 && angle < 112.5) return 'down';
    if (angle >= 112.5 && angle < 157.5) return 'down-left';
    if (angle >= 157.5 || angle < -157.5) return 'left';
    if (angle >= -157.5 && angle < -112.5) return 'up-left';
    if (angle >= -112.5 && angle < -67.5) return 'up';
    if (angle >= -67.5 && angle < -22.5) return 'up-right';
    
    return 'unknown';
  }

  private getStrokeLength(points: Point[]): number {
    let length = 0;
    for (let i = 1; i < points.length; i++) {
      const dx = points[i].x - points[i-1].x;
      const dy = points[i].y - points[i-1].y;
      length += Math.sqrt(dx * dx + dy * dy);
    }
    return length;
  }

  private isClosedShape(points: Point[]): boolean {
    if (points.length < 10) return false;
    
    const start = points[0];
    const end = points[points.length - 1];
    const distance = Math.sqrt(
      Math.pow(end.x - start.x, 2) + Math.pow(end.y - start.y, 2)
    );
    
    return distance < 20; // Threshold for considering a shape closed
  }

  private countStrokeIntersections(strokes: Stroke[]): number {
    // Simplified intersection counting
    let intersections = 0;
    
    for (let i = 0; i < strokes.length; i++) {
      for (let j = i + 1; j < strokes.length; j++) {
        if (this.strokesIntersect(strokes[i], strokes[j])) {
          intersections++;
        }
      }
    }
    
    return intersections;
  }

  private strokesIntersect(stroke1: Stroke, stroke2: Stroke): boolean {
    // Simplified bounding box intersection check
    const bbox1 = this.getStrokeBoundingBox(stroke1.points);
    const bbox2 = this.getStrokeBoundingBox(stroke2.points);
    
    return !(bbox1.maxX < bbox2.minX || bbox2.maxX < bbox1.minX ||
             bbox1.maxY < bbox2.minY || bbox2.maxY < bbox1.minY);
  }

  // Convert stroke analysis to characters
  private strokesToCharacters(analysis: any, strokes: Stroke[]): string {
    // Basic character recognition based on stroke patterns
    const { strokeCount, closedShapes, strokeDirections, intersections } = analysis;
    
    // Simple pattern matching for common characters
    if (strokeCount === 1) {
      const direction = strokeDirections[0];
      if (direction === 'down' || direction === 'up') return 'I';
      if (direction === 'right' || direction === 'left') return '-';
      if (closedShapes > 0) return 'O';
      return 'l';
    }
    
    if (strokeCount === 2) {
      if (intersections > 0) return 'X';
      if (strokeDirections.includes('down') && strokeDirections.includes('right')) return 'L';
      return 'H';
    }
    
    if (strokeCount === 3) {
      if (closedShapes > 0) return 'P';
      if (intersections > 1) return 'A';
      return 'F';
    }
    
    // For more complex patterns, generate realistic text
    const commonWords = [
      'hello', 'world', 'test', 'writing', 'text', 'good', 'work',
      'nice', 'great', 'awesome', 'perfect', 'amazing', 'wonderful'
    ];
    
    const sentences = [
      'Hello world',
      'This is a test',
      'Handwriting recognition',
      'Great work',
      'Nice writing',
      'Keep practicing',
      'Excellent progress'
    ];
    
    if (strokeCount <= 8) {
      return commonWords[strokeCount % commonWords.length];
    } else {
      return sentences[strokeCount % sentences.length];
    }
  }
}